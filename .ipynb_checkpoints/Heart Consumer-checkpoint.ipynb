{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2ac3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "import json\n",
    "from pyspark.sql.functions import col, sum as spark_sum, when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b02b553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 09:30:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Configure the Kafka consumer\n",
    "consumer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'heart_consumer_group',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "consumer = Consumer(consumer_config)\n",
    "spark = SparkSession.builder.appName(\"HeartDiseaseProcessing\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9340785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscribed to topic: ('heart_data', 'test')\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Subscribe to the Kafka topic\n",
    "topic1 = 'heart_data'\n",
    "topic2='test'\n",
    "consumer.subscribe([topic1,topic2])\n",
    "\n",
    "print(f\"Subscribed to topic: {topic1,topic2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56bb7ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Variable to store received data\n",
    "received_rdd = spark.sparkContext.emptyRDD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa479723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_messages():\n",
    "    global received_rdd \n",
    "    try:\n",
    "        while True:\n",
    "            msg = consumer.poll(1.0)  # Poll Kafka for new messages\n",
    "            \n",
    "            if msg is None:\n",
    "                continue  # No new message, continue polling\n",
    "            if msg.error():\n",
    "                print(f\"Consumer error: {msg.error()}\") \n",
    "                continue\n",
    "            topic = msg.topic()\n",
    "            if topic != \"heart_data\":\n",
    "                continue\n",
    "                \n",
    "            # Decode the received message\n",
    "            data = json.loads(msg.value().decode('utf-8'))\n",
    "            \n",
    "            # Check for termination signal\n",
    "            if isinstance(data, dict) and data.get(\"end_of_stream\"):\n",
    "                print(\"End of stream signal received. Stopping consumer...\")\n",
    "                break\n",
    "            \n",
    "            # Convert batch to RDD and append to existing RDD\n",
    "            batch_rdd = spark.sparkContext.parallelize(data)  \n",
    "            received_rdd = received_rdd.union(batch_rdd)\n",
    "\n",
    "            print(f\"Received batch with {len(data)} records, Total stored in RDD: {received_rdd.count()}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Consumer interrupted. Closing connection...\")\n",
    "    finally:\n",
    "        #consumer.close()\n",
    "        print(f\"Total number of records received in RDD: {received_rdd.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08db70ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer error: KafkaError{code=UNKNOWN_TOPIC_OR_PART,val=3,str=\"Subscribed topic not available: heart_data: Broker: Unknown topic or partition\"}\n",
      "Consumer error: KafkaError{code=UNKNOWN_TOPIC_OR_PART,val=3,str=\"Subscribed topic not available: test: Broker: Unknown topic or partition\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received batch with 50 records, Total stored in RDD: 50\n",
      "Received batch with 50 records, Total stored in RDD: 100\n",
      "Received batch with 50 records, Total stored in RDD: 150\n",
      "Received batch with 50 records, Total stored in RDD: 200\n",
      "Received batch with 50 records, Total stored in RDD: 250\n",
      "Received batch with 50 records, Total stored in RDD: 300\n",
      "Received batch with 50 records, Total stored in RDD: 350\n",
      "Received batch with 50 records, Total stored in RDD: 400\n",
      "Received batch with 50 records, Total stored in RDD: 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:===============================================>         (33 + 4) / 40]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received batch with 50 records, Total stored in RDD: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:=============================================>          (36 + 4) / 44]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received batch with 50 records, Total stored in RDD: 550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:============================================>           (38 + 4) / 48]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received batch with 50 records, Total stored in RDD: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received batch with 50 records, Total stored in RDD: 650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received batch with 50 records, Total stored in RDD: 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received batch with 50 records, Total stored in RDD: 750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received batch with 50 records, Total stored in RDD: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received batch with 50 records, Total stored in RDD: 850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received batch with 50 records, Total stored in RDD: 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received batch with 20 records, Total stored in RDD: 920\n",
      "End of stream signal received. Stopping consumer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=====================================================>  (73 + 3) / 76]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records received in RDD: 920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "consume_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2308a63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---------------+---------+-----+-----+---+---+-------+--------------+------+-----------+-----------------+------+--------+\n",
      "|age| ca| chol|             cp|  dataset|exang|  fbs| id|num|oldpeak|       restecg|   sex|      slope|             thal|thalch|trestbps|\n",
      "+---+---+-----+---------------+---------+-----+-----+---+---+-------+--------------+------+-----------+-----------------+------+--------+\n",
      "| 63|0.0|233.0| typical angina|Cleveland|false| true|  1|  0|    2.3|lv hypertrophy|  Male|downsloping|     fixed defect| 150.0|   145.0|\n",
      "| 67|3.0|286.0|   asymptomatic|Cleveland| true|false|  2|  2|    1.5|lv hypertrophy|  Male|       flat|           normal| 108.0|   160.0|\n",
      "| 67|2.0|229.0|   asymptomatic|Cleveland| true|false|  3|  1|    2.6|lv hypertrophy|  Male|       flat|reversable defect| 129.0|   120.0|\n",
      "| 37|0.0|250.0|    non-anginal|Cleveland|false|false|  4|  0|    3.5|        normal|  Male|downsloping|           normal| 187.0|   130.0|\n",
      "| 41|0.0|204.0|atypical angina|Cleveland|false|false|  5|  0|    1.4|lv hypertrophy|Female|  upsloping|           normal| 172.0|   130.0|\n",
      "+---+---+-----+---------------+---------+-----+-----+---+---+-------+--------------+------+-----------+-----------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(received_rdd)\n",
    "df.show(5)  # Display first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a637dbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- ca: double (nullable = true)\n",
      " |-- chol: double (nullable = true)\n",
      " |-- cp: string (nullable = true)\n",
      " |-- dataset: string (nullable = true)\n",
      " |-- exang: boolean (nullable = true)\n",
      " |-- fbs: boolean (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- num: long (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- restecg: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- slope: string (nullable = true)\n",
      " |-- thal: string (nullable = true)\n",
      " |-- thalch: double (nullable = true)\n",
      " |-- trestbps: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4171674a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, age: string, ca: string, chol: string, cp: string, dataset: string, id: string, num: string, oldpeak: string, restecg: string, sex: string, slope: string, thal: string, thalch: string, trestbps: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33ba806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset Description:\n",
    "\n",
    "#| Variable   | Description                                                                                   |\n",
    "#|------------|-----------------------------------------------------------------------------------------------|\n",
    "#| age        | Age of the patient in years                                                                   |\n",
    "#| sex        | Gender of the patient (0 = male, 1 = female)                                                  |\n",
    "#| cp         | Chest pain type: 0: Typical angina, 1: Atypical angina, 2: Non-anginal pain, 3: Asymptomatic |\n",
    "#| trestbps   | Resting blood pressure in mm Hg                                                               |\n",
    "#| chol       | Serum cholesterol in mg/dl                                                                    |\n",
    "#| fbs        | Fasting blood sugar level, categorized as above 120 mg/dl (1 = true, 0 = false)               |\n",
    "#| restecg    | Resting electrocardiographic results: 0: Normal, 1: Having ST-T wave abnormality, 2: Showing probable or definite left ventricular hypertrophy |\n",
    "#| thalach    | Maximum heart rate achieved during a stress test                                              |\n",
    "#| exang      | Exercise-induced angina (1 = yes, 0 = no)                                                     |\n",
    "#| oldpeak    | ST depression induced by exercise relative to rest                                             |\n",
    "#| slope      | Slope of the peak exercise ST segment: 0: Upsloping, 1: Flat, 2: Downsloping                   |\n",
    "#| ca         | Number of major vessels (0-4) colored by fluoroscopy                                           |\n",
    "#| thal       | Thalium stress test result: 0: Normal, 1: Fixed defect, 2: Reversible defect, 3: Not described |\n",
    "#| target     | Heart disease status ( 0 = no heart disease.\n",
    "#                                      1 = Mild Heart Disease types.\n",
    "#                                      2 = Moderate Heart Disease type.\n",
    "#                                      3 =  Severe Heart Disease type.\n",
    "#                                      4 =  Critical Heart Disease type.)                                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa9fc1",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43410553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/linuxu/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2863, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2909, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3106, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3309, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_192357/2870131861.py\", line 1, in <cell line: 1>\n",
      "    import seaborn as sns\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/seaborn/__init__.py\", line 2, in <module>\n",
      "    from .rcmod import *  # noqa: F401,F403\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/seaborn/rcmod.py\", line 5, in <module>\n",
      "    import matplotlib as mpl\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py\", line 109, in <module>\n",
      "    from . import _api, _version, cbook, docstring, rcsetup\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, cbook, scale\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/matplotlib/scale.py\", line 23, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/matplotlib/ticker.py\", line 136, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/matplotlib/transforms.py\", line 46, in <module>\n",
      "    from matplotlib._path import (\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/seaborn/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import seaborn objects\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrcmod\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpalettes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/seaborn/rcmod.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcycler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycler\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m palettes\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py:109\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, docstring, rcsetup\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning, sanitize_sequence\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mplDeprecation  \u001b[38;5;66;03m# deprecated\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/rcsetup.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/colors.py:56\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook, scale\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_color_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ColorMapping\u001b[39;00m(\u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/scale.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, docstring\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     NullFormatter, ScalarFormatter, LogFormatterSciNotation, LogitFormatter,\n\u001b[1;32m     25\u001b[0m     NullLocator, LogLocator, AutoLocator, AutoMinorLocator,\n\u001b[1;32m     26\u001b[0m     SymmetricalLogLocator, LogitLocator)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transform, IdentityTransform\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mScaleBase\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/ticker.py:136\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m mtransforms\n\u001b[1;32m    138\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    140\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTickHelper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFixedFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    141\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNullFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuncFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatStrFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    142\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrMethodFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScalarFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultipleLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxNLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoMinorLocator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    149\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymmetricalLogLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogitLocator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/transforms.py:46\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inv\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_path\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     47\u001b[0m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     50\u001b[0m DEBUG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc0051",
   "metadata": {},
   "source": [
    "# Age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfdc735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, avg, stddev\n",
    "\n",
    "df.select(\n",
    "    min(\"age\").alias(\"Min Age\"),\n",
    "    max(\"age\").alias(\"Max Age\"),\n",
    "    avg(\"age\").alias(\"Average Age\"),\n",
    "    stddev(\"age\").alias(\"Std Dev Age\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'age' column from PySpark DataFrame to Pandas DataFrame\n",
    "df_pandas = df.select(\"age\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e4bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate central tendency metrics\n",
    "mean_age = df_pandas[\"age\"].mean()   # Mean (Average)\n",
    "median_age = df_pandas[\"age\"].median()  # Median\n",
    "mode_age = df_pandas[\"age\"].mode()[0]  # Mode (first mode if multiple exist)\n",
    "\n",
    "# Create a histogram with KDE (Kernel Density Estimate)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_pandas[\"age\"], kde=True, bins=30, color=\"skyblue\")\n",
    "\n",
    "#  Add vertical lines for Mean, Median, and Mode\n",
    "plt.axvline(mean_age, color='red', linestyle='dashed', linewidth=2, label=f\"Mean: {mean_age:.2f}\")\n",
    "plt.axvline(median_age, color='green', linestyle='dashed', linewidth=2, label=f\"Median: {median_age:.2f}\")\n",
    "plt.axvline(mode_age, color='blue', linestyle='dashed', linewidth=2, label=f\"Mode: {mode_age:.2f}\")\n",
    "\n",
    "#  Customize the plot with titles and labels\n",
    "plt.title(\"Age Distribution with Mean, Median & Mode\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()  # Display the legend\n",
    "plt.show()  # Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark DataFrame to Pandas\n",
    "df_pandas = df.select(\"age\", \"num\").toPandas()\n",
    "\n",
    "# Calculate the mean age\n",
    "mean_age = df_pandas[\"age\"].mean()\n",
    "\n",
    "# Define color mapping for all 5 heart disease classes (0-4)\n",
    "color_mapping = {0: \"blue\", 1: \"red\", 2: \"orange\", 3: \"purple\", 4: \"green\"}\n",
    "labels_mapping = {\n",
    "    0: \"No Disease (0)\",\n",
    "    1: \"Heart Disease Type 1\",\n",
    "    2: \"Heart Disease Type 2\",\n",
    "    3: \"Heart Disease Type 3\",\n",
    "    4: \"Heart Disease Type 4\"\n",
    "}\n",
    "\n",
    "# Histogram with Multi-Class Distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df_pandas, x=\"age\", hue=\"num\", multiple=\"stack\", bins=30, palette=color_mapping)\n",
    "\n",
    "# Add an indicator line at the mean age\n",
    "plt.axvline(mean_age, color='black', linestyle='dashed', linewidth=2, label=f\"Mean Age: {mean_age:.2f}\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Heart Disease Distribution by Age (Multi-Class: 0-4)\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Modify the legend to include color explanations\n",
    "handles = [plt.Line2D([0], [0], color=color_mapping[key], lw=6) for key in labels_mapping]\n",
    "plt.legend(handles, labels_mapping.values(), title=\"Heart Disease Class\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf5f80a",
   "metadata": {},
   "source": [
    "Key Insights from Age Graphs\n",
    "#Heart disease is more common in older individuals (50+), especially severe cases (num = 3-4).\n",
    "#Younger people (<40) mostly fall in the num = 0 (healthy) category, with fewer cases of disease.\n",
    "#The highest risk group appears to be between ages 55-65.\n",
    "#Preventive care is crucial in middle age to reduce risk.\n",
    "#Let me know if you need further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1dffb8",
   "metadata": {},
   "source": [
    "# Gender column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark DataFrame to Pandas\n",
    "df_pandas = df.select(\"sex\").toPandas()\n",
    "\n",
    "# Count the number of males and females\n",
    "gender_counts = df_pandas[\"sex\"].value_counts()\n",
    "\n",
    "# Bar plot for gender distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=gender_counts.index, y=gender_counts.values, palette=\"pastel\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Gender Distribution in Dataset\")\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Show value labels on bars\n",
    "for i, value in enumerate(gender_counts.values):\n",
    "    plt.text(i, value + 2, str(value), ha='center', fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print gender distribution\n",
    "print(gender_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257fc721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark DataFrame to Pandas\n",
    "df_pandas = df.select(\"sex\", \"num\").toPandas()\n",
    "\n",
    "# Countplot: Heart Disease Cases by Gender\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=df_pandas, x=\"sex\", hue=\"num\", palette=\"Set2\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Heart Disease Distribution by Gender\")\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Heart Disease Class\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a10d141",
   "metadata": {},
   "source": [
    "Males are 274.23% more than females in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebde6652",
   "metadata": {},
   "source": [
    "# Cp column \n",
    "**Types of Chest pain :**\n",
    "\n",
    "    1. Asymptomatic: No chest pain or discomfort.\n",
    "    2. Non-Anginal: Chest pain not typical of heart-related issues; requires further investigation.\n",
    "    3. Atypical Angina: Chest pain with characteristics different from typical heart-related chest pain.\n",
    "    4. Typical Angina: Classic chest pain indicating potential insufficient blood supply to the heart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4bd46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark DataFrame to Pandas\n",
    "df_pandas = df.select(\"cp\").toPandas()\n",
    "\n",
    "# Count occurrences of each chest pain type\n",
    "cp_counts = df_pandas[\"cp\"].value_counts()\n",
    "\n",
    "# Bar plot for chest pain type distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=cp_counts.index, y=cp_counts.values, palette=\"viridis\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Chest Pain Type Distribution\")\n",
    "plt.xlabel(\"Chest Pain Type (cp)\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Show value labels on bars\n",
    "for i, value in enumerate(cp_counts.values):\n",
    "    plt.text(i, value + 2, str(value), ha='center', fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print cp distribution\n",
    "print(cp_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d890f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark DataFrame to Pandas\n",
    "df_pandas = df.select(\"cp\", \"num\").toPandas()\n",
    "\n",
    "# Countplot: Distribution of Heart Disease by Chest Pain Type\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=df_pandas, x=\"cp\", hue=\"num\", palette=\"Set2\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Heart Disease Severity by Chest Pain Type\")\n",
    "plt.xlabel(\"Chest Pain Type (cp)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Heart Disease Class\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97730cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48861f31",
   "metadata": {},
   "source": [
    "#  Trestbps column\n",
    "The normal resting blood pressure is 120/80 mm Hg.\n",
    "\n",
    "high blood pressure increasing the risk of heart disease and stroke, often asymptomatic, while low blood pressure can lead to dizziness and fainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f262cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark DataFrame to Pandas\n",
    "df_pandas = df.select(\"trestbps\", \"num\").toPandas()\n",
    "\n",
    "# Histogram: Distribution of Resting Blood Pressure\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df_pandas[\"trestbps\"], bins=30, kde=True, color=\"skyblue\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Distribution of Resting Blood Pressure (trestbps)\")\n",
    "plt.xlabel(\"Resting Blood Pressure (mmHg)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aaa334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark DataFrame to Pandas\n",
    "df_pandas = df.select(\"age\", \"trestbps\", \"num\").toPandas()\n",
    "\n",
    "# Filter only people with heart disease (`num > 0`)\n",
    "df_heart_disease = df_pandas[df_pandas[\"num\"] > 0]\n",
    "\n",
    "# Group by age and calculate average resting blood pressure\n",
    "bp_by_age = df_heart_disease.groupby(\"age\")[\"trestbps\"].mean().reset_index()\n",
    "\n",
    "# 1️ Line plot: Mean Blood Pressure by Age for Heart Disease Patients\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(data=bp_by_age, x=\"age\", y=\"trestbps\", marker=\"o\", color=\"red\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Average Resting Blood Pressure by Age (Heart Disease Patients)\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Mean Resting Blood Pressure (mmHg)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f438d3a2",
   "metadata": {},
   "source": [
    "**This graph shows how average blood pressure changes with age among heart disease patients.\n",
    "we see higher blood pressure in older age groups.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3759b4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67938180",
   "metadata": {},
   "source": [
    "# Dataset Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark DataFrame to Pandas\n",
    "df_pandas = df.select(\"dataset\").toPandas()\n",
    "\n",
    "# Count occurrences of each dataset source\n",
    "dataset_counts = df_pandas[\"dataset\"].value_counts()\n",
    "\n",
    "#  Bar plot for dataset distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=dataset_counts.index, y=dataset_counts.values, palette=\"pastel\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Dataset Source Distribution\")\n",
    "plt.xlabel(\"Dataset Source\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Show value labels on bars\n",
    "for i, value in enumerate(dataset_counts.values):\n",
    "    plt.text(i, value + 2, str(value), ha='center', fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print dataset source distribution\n",
    "print(dataset_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1541ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark DataFrame to Pandas\n",
    "df_pandas = df.select(\"dataset\", \"num\").toPandas()\n",
    "\n",
    "#  Countplot: Distribution of Heart Disease Classes by Dataset\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=df_pandas, x=\"dataset\", hue=\"num\", palette=\"Set2\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Heart Disease Severity Across Different Datasets\")\n",
    "plt.xlabel(\"Dataset Source\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Heart Disease Class\")\n",
    "plt.xticks(rotation=15)  # Rotate labels if necessary\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101956b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas (numeric columns only)\n",
    "numeric_columns = [\"age\", \"trestbps\", \"chol\", \"thalch\", \"oldpeak\", \"ca\", \"num\"]\n",
    "df_pandas = df.select(numeric_columns).toPandas()\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = df_pandas.corr()\n",
    "\n",
    "# 1️⃣ Heatmap for Correlation Matrix\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa186248",
   "metadata": {},
   "source": [
    "# Conclusions Based on Our Analysis\n",
    "**1. Age and Heart Disease  \n",
    "    *The minimum recorded age for heart disease in the dataset is 28 years old.  \n",
    "    *Heart disease cases peak between 53-55 years old, affecting both males and females.  \n",
    "    *Older individuals (55+) have a higher probability of severe heart disease (num = 3, 4).**\n",
    "\n",
    "**2. Gender Differences  \n",
    "    *Males (78.91%) significantly outnumber females (21.09%) in the dataset.  \n",
    "    *Males have a higher prevalence of severe heart disease, while females have fewer cases overall.**\n",
    "\n",
    "**3. Chest Pain Type (cp) and Heart Disease  \n",
    "    *Asymptomatic chest pain (cp = asymptomatic) is the most common among individuals with severe heart disease.  \n",
    "    *197 individuals with mild heart disease had no chest pain symptoms, highlighting the risk of undiagnosed heart disease.  \n",
    "    *The majority of people with critical heart disease (num = 4) reported asymptomatic or atypical angina.**\n",
    "\n",
    "**4. Resting Blood Pressure (trestbps) and Heart Disease  \n",
    "    *Higher resting blood pressure is associated with more severe heart disease.\n",
    "    *Individuals above 50 years old with high blood pressure are more likely to have severe heart disease (num = 3, 4).   \n",
    "    *The mean blood pressure tends to increase with age, showing that hypertension is a key risk factor.**\n",
    "\n",
    "**5. Dataset Variability and Generalizability  \n",
    "    *Most patients come from Cleveland (304) and the least from Switzerland (123).**\n",
    "\n",
    "# Final Observations\n",
    "**Age, gender, chest pain type, and blood pressure are key predictors of heart disease.\n",
    "Undiagnosed or asymptomatic cases pose a major risk, especially in older patients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b18f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c249204",
   "metadata": {},
   "source": [
    "# Preprocessing the data and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99b28159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===============================================>        (64 + 4) / 76]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---+-------+-----+---+---+---+-------+-------+---+-----+----+------+--------+\n",
      "|age| ca|chol| cp|dataset|exang|fbs| id|num|oldpeak|restecg|sex|slope|thal|thalch|trestbps|\n",
      "+---+---+----+---+-------+-----+---+---+---+-------+-------+---+-----+----+------+--------+\n",
      "|  0|611|  30|  0|      0|   55| 90|  0|  0|     62|      2|  0|  309| 486|    55|      59|\n",
      "+---+---+----+---+-------+-----+---+---+---+-------+-------+---+-----+----+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    *[spark_sum(when(col(c).isNull() | (col(c) == \"\") | (col(c) == \"NaN\"), 1).otherwise(0)).alias(c) for c in df.columns]\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82933d5c",
   "metadata": {},
   "source": [
    "# Summary: Why We Dropped These Columns\n",
    "\n",
    "**ca- Number of major vessels seen in fluoroscopy\t611 missing values (too much data missing)  \n",
    "slope-Change in ST segment during exercise 309 missing values (loss of key medical data)  \n",
    "thal-Thalassemia blood disorder type\t486 missing values (incomplete data for analysis)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a109679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------------+---------+-----+-----+---+---+-------+--------------+------+------+--------+\n",
      "|age| chol|             cp|  dataset|exang|  fbs| id|num|oldpeak|       restecg|   sex|thalch|trestbps|\n",
      "+---+-----+---------------+---------+-----+-----+---+---+-------+--------------+------+------+--------+\n",
      "| 63|233.0| typical angina|Cleveland|false| true|  1|  0|    2.3|lv hypertrophy|  Male| 150.0|   145.0|\n",
      "| 67|286.0|   asymptomatic|Cleveland| true|false|  2|  2|    1.5|lv hypertrophy|  Male| 108.0|   160.0|\n",
      "| 67|229.0|   asymptomatic|Cleveland| true|false|  3|  1|    2.6|lv hypertrophy|  Male| 129.0|   120.0|\n",
      "| 37|250.0|    non-anginal|Cleveland|false|false|  4|  0|    3.5|        normal|  Male| 187.0|   130.0|\n",
      "| 41|204.0|atypical angina|Cleveland|false|false|  5|  0|    1.4|lv hypertrophy|Female| 172.0|   130.0|\n",
      "+---+-----+---------------+---------+-----+-----+---+---+-------+--------------+------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define columns to drop (too many missing values)\n",
    "columns_to_drop = [\"ca\", \"slope\", \"thal\"]\n",
    "\n",
    "#  Drop these columns from the DataFrame\n",
    "df_cleaned = df.drop(*columns_to_drop)\n",
    "\n",
    "#  Show the updated DataFrame after removing columns\n",
    "df_cleaned.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "debc3183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxu/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 2.0.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/linuxu/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2863, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2909, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3106, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3309, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_192357/2456385120.py\", line 1, in <cell line: 1>\n",
      "    from pyspark.ml.feature import Imputer\n",
      "  File \"/usr/local/spark/spark/python/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import (\n",
      "  File \"/usr/local/spark/spark/python/pyspark/ml/base.py\", line 40, in <module>\n",
      "    from pyspark.ml.param import P\n",
      "  File \"/usr/local/spark/spark/python/pyspark/ml/param/__init__.py\", line 35, in <module>\n",
      "    from pyspark.ml.linalg import DenseVector, Vector, Matrix\n",
      "  File \"/usr/local/spark/spark/python/pyspark/ml/linalg/__init__.py\", line 81, in <module>\n",
      "    import scipy.sparse\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/scipy/sparse/__init__.py\", line 228, in <module>\n",
      "    from .csr import *\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/scipy/sparse/csr.py\", line 10, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.functions import col, when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cde2291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\"chol\", \"oldpeak\", \"thalch\", \"trestbps\"] # Numeric features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "407a2250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "imputer = Imputer(inputCols=numeric_columns, \n",
    "                  outputCols=[f\"{col}_imputed\" for col in numeric_columns]).setStrategy(\"mean\")\n",
    "# Apply imputer to numerical columns\n",
    "df_filled = imputer.fit(df_cleaned).transform(df_cleaned)\n",
    "df_filled=df_filled.drop(*numeric_columns)\n",
    "# Drop original numeric columns and rename imputed ones\n",
    "for col_name in numeric_columns:\n",
    "    df_imputed = df_filled.drop(col_name).withColumnRenamed(f\"{col_name}_imputed\", col_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3082b7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:============================================>           (61 + 4) / 76]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+-----+---+---+---+-------+---+------------+---------------+--------------+--------+\n",
      "|age| cp|dataset|exang|fbs| id|num|restecg|sex|chol_imputed|oldpeak_imputed|thalch_imputed|trestbps|\n",
      "+---+---+-------+-----+---+---+---+-------+---+------------+---------------+--------------+--------+\n",
      "|  0|  0|      0|   55| 90|  0|  0|      2|  0|           0|              0|             0|       0|\n",
      "+---+---+-------+-----+---+---+---+-------+---+------------+---------------+--------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 34:=====================================================>  (73 + 3) / 76]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_missing_values = df_imputed.select(\n",
    "    *[spark_sum(when(col(c).isNull() | (col(c) == \"\") | (col(c) == \"NaN\"), 1).otherwise(0)).alias(c) for c in df_imputed.columns]\n",
    ")\n",
    "\n",
    "# Display missing values per column\n",
    "df_missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "872bcee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert \"TRUE\"/\"FALSE\" values in `fbs` and `exang` to 1/0\n",
    "binary_columns = [\"fbs\", \"exang\"]\n",
    "\n",
    "for col_name in binary_columns:\n",
    "    df_imputed = df_imputed.withColumn(col_name, when(col(col_name) == \"TRUE\", 1).otherwise(0))\n",
    "\n",
    "# Step 2: Fill missing values in `fbs` and `exang` with the most frequent value (mode)\n",
    "for col_name in binary_columns:\n",
    "    mode_value = df_imputed.groupBy(col_name).count().orderBy(col(\"count\").desc()).first()[0]\n",
    "    df_imputed = df_imputed.withColumn(col_name, when(col(col_name).isNull(), mode_value).otherwise(col(col_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01d04aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:================================================>       (66 + 4) / 76]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+-----+---+---+---+-------+---+------------+---------------+--------------+--------+\n",
      "|age| cp|dataset|exang|fbs| id|num|restecg|sex|chol_imputed|oldpeak_imputed|thalch_imputed|trestbps|\n",
      "+---+---+-------+-----+---+---+---+-------+---+------------+---------------+--------------+--------+\n",
      "|  0|  0|      0|    0|  0|  0|  0|      2|  0|           0|              0|             0|       0|\n",
      "+---+---+-------+-----+---+---+---+-------+---+------------+---------------+--------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum, when\n",
    "\n",
    "df_missing_values = df_imputed.select(\n",
    "    *[spark_sum(when(col(c).isNull() | (col(c) == \"\") | (col(c) == \"NaN\"), 1).otherwise(0)).alias(c) for c in df_imputed.columns]\n",
    ")\n",
    "\n",
    "# Display missing values per column\n",
    "df_missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f59e927b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:===============================================>        (65 + 4) / 76]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|sex|\n",
      "+---+\n",
      "|  1|\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert \"Male\" to 1 and \"Female\" to 0\n",
    "df_imputed = df_imputed.withColumn(\"sex\", when(col(\"sex\") == \"Male\", 1).otherwise(0))\n",
    "\n",
    "# Show the result to verify\n",
    "df_imputed.select(\"sex\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df000409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+---+---+---+------------+---------------+--------------+--------+-------------+-------------+--------+\n",
      "|age|exang|fbs| id|num|sex|chol_imputed|oldpeak_imputed|thalch_imputed|trestbps|dataset_index|restecg_index|cp_index|\n",
      "+---+-----+---+---+---+---+------------+---------------+--------------+--------+-------------+-------------+--------+\n",
      "| 63|    0|  1|  1|  0|  1|       233.0|            2.3|         150.0|   145.0|          0.0|          1.0|     3.0|\n",
      "| 67|    1|  0|  2|  2|  1|       286.0|            1.5|         108.0|   160.0|          0.0|          1.0|     0.0|\n",
      "| 67|    1|  0|  3|  1|  1|       229.0|            2.6|         129.0|   120.0|          0.0|          1.0|     0.0|\n",
      "| 37|    0|  0|  4|  0|  1|       250.0|            3.5|         187.0|   130.0|          0.0|          0.0|     1.0|\n",
      "| 41|    0|  0|  5|  0|  0|       204.0|            1.4|         172.0|   130.0|          0.0|          1.0|     2.0|\n",
      "+---+-----+---+---+---+---+------------+---------------+--------------+--------+-------------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Step 1: Define categorical columns to be converted into numerical values\n",
    "categorical_columns = [\"dataset\", \"restecg\",\"cp\"]\n",
    "\n",
    "# Step 2: Apply StringIndexer to convert categorical values into numerical indices\n",
    "indexers = [StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_index\").setHandleInvalid(\"keep\") for col_name in categorical_columns]\n",
    "\n",
    "# Step 3: Create a pipeline to apply transformations\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "\n",
    "# Step 4: Fit and transform the dataset\n",
    "df_indexed = pipeline.fit(df_imputed).transform(df_imputed)\n",
    "\n",
    "# Step 5: Drop original categorical columns and keep only indexed ones\n",
    "df_indexed = df_indexed.drop(*categorical_columns)\n",
    "\n",
    "# Step 6: Show the transformed dataset\n",
    "df_indexed.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "753f1c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 65:===============================================>        (64 + 4) / 76]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping for dataset:\n",
      "  Cleveland -> 0\n",
      "  Hungary -> 1\n",
      "  VA Long Beach -> 2\n",
      "  Switzerland -> 3\n",
      "Mapping for restecg:\n",
      "  normal -> 0\n",
      "  lv hypertrophy -> 1\n",
      "  st-t abnormality -> 2\n",
      "  NaN -> 3\n",
      "Mapping for cp:\n",
      "  asymptomatic -> 0\n",
      "  non-anginal -> 1\n",
      "  atypical angina -> 2\n",
      "  typical angina -> 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 65:=====================================================>  (72 + 4) / 76]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# יצירת מילון להתאמה בין קטגוריות למספרים\n",
    "category_mappings = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=f\"{col}_index\")\n",
    "    model = indexer.fit(df_imputed)\n",
    "    labels = model.labels\n",
    "    category_mappings[col] = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "# הצגת ההתאמה לכל עמודה קטגורית\n",
    "for col, mapping in category_mappings.items():\n",
    "    print(f\"Mapping for {col}:\")\n",
    "    for label, index in mapping.items():\n",
    "        print(f\"  {label} -> {index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ac2efb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, exang: int, fbs: int, id: bigint, num: bigint, sex: int, chol_imputed: double, oldpeak_imputed: double, thalch_imputed: double, trestbps: double, dataset_index: double, restecg_index: double, cp_index: double]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_indexed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dde09be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping for dataset: ['Cleveland', 'Hungary', 'VA Long Beach', 'Switzerland']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping for restecg: ['normal', 'lv hypertrophy', 'st-t abnormality', 'NaN']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:=====================================================>  (73 + 3) / 76]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping for cp: ['asymptomatic', 'non-anginal', 'atypical angina', 'typical angina']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for indexer in indexers:\n",
    "    model = indexer.fit(df_imputed)\n",
    "    labels = model.labels\n",
    "    print(f\"Mapping for {indexer.getInputCol()}: {labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdf39122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 77:===============================================>        (65 + 4) / 76]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+---+---+---+------------+---------------+--------------+--------+-------------+-------------+--------+\n",
      "|age|exang|fbs| id|num|sex|chol_imputed|oldpeak_imputed|thalch_imputed|trestbps|dataset_index|restecg_index|cp_index|\n",
      "+---+-----+---+---+---+---+------------+---------------+--------------+--------+-------------+-------------+--------+\n",
      "|  0|    0|  0|  0|  0|  0|           0|              0|             0|       0|            0|            0|       0|\n",
      "+---+-----+---+---+---+---+------------+---------------+--------------+--------+-------------+-------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum, when\n",
    "\n",
    "df_missing_values = df_indexed.select(\n",
    "    *[spark_sum(when(col(c).isNull() | (col(c) == \"\") | (col(c) == \"NaN\"), 1).otherwise(0)).alias(c) for c in df_indexed.columns]\n",
    ")\n",
    "\n",
    "# Display missing values per column\n",
    "df_missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fabab214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- exang: integer (nullable = false)\n",
      " |-- fbs: integer (nullable = false)\n",
      " |-- id: long (nullable = true)\n",
      " |-- num: long (nullable = true)\n",
      " |-- sex: integer (nullable = false)\n",
      " |-- chol_imputed: double (nullable = true)\n",
      " |-- oldpeak_imputed: double (nullable = true)\n",
      " |-- thalch_imputed: double (nullable = true)\n",
      " |-- trestbps: double (nullable = true)\n",
      " |-- dataset_index: double (nullable = false)\n",
      " |-- restecg_index: double (nullable = false)\n",
      " |-- cp_index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_indexed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7956d9cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Sample data columns based on the schema\u001b[39;00m\n\u001b[1;32m      2\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexang\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfbs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchol_imputed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moldpeak_imputed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthalch_imputed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrestbps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestecg_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcp_index\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m df_pandas \u001b[38;5;241m=\u001b[39m \u001b[43mdf_indexed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compute correlation matrix\u001b[39;00m\n\u001b[1;32m      8\u001b[0m correlation_matrix \u001b[38;5;241m=\u001b[39m df_pandas\u001b[38;5;241m.\u001b[39mcorr()\n",
      "File \u001b[0;32m/usr/local/spark/spark/python/pyspark/sql/pandas/conversion.py:86\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, DataFrame)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[0;32m---> 86\u001b[0m \u001b[43mrequire_minimum_pandas_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/spark/python/pyspark/sql/pandas/utils.py:27\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     have_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/__init__.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m hard_dependencies, dependency, missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/compat/__init__.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     is_numpy_dev,\n\u001b[1;32m     17\u001b[0m     np_version_under1p19,\n\u001b[1;32m     18\u001b[0m     np_version_under1p20,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     pa_version_under1p01,\n\u001b[1;32m     22\u001b[0m     pa_version_under2p0,\n\u001b[1;32m     23\u001b[0m     pa_version_under3p0,\n\u001b[1;32m     24\u001b[0m     pa_version_under4p0,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m PY39 \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m9\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/compat/numpy/__init__.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[1;32m      7\u001b[0m _np_version \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     Appender,\n\u001b[1;32m      3\u001b[0m     Substitution,\n\u001b[1;32m      4\u001b[0m     cache_readonly,\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     hash_array,\n\u001b[1;32m      9\u001b[0m     hash_pandas_object,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     Any,\n\u001b[1;32m      8\u001b[0m     Callable,\n\u001b[1;32m      9\u001b[0m     Mapping,\n\u001b[1;32m     10\u001b[0m     cast,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperties\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cache_readonly  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeprecate\u001b[39m(\n\u001b[1;32m     19\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     20\u001b[0m     alternative: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     26\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable[[F], F]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/__init__.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     NaT,\n\u001b[1;32m     16\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     iNaT,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# Sample data columns based on the schema\n",
    "columns = [\"age\", \"exang\", \"fbs\", \"id\", \"num\", \"sex\", \"chol_imputed\", \"oldpeak_imputed\",\n",
    "           \"thalch_imputed\", \"trestbps\", \"dataset_index\", \"restecg_index\", \"cp_index\"]\n",
    "\n",
    "df_pandas = df_indexed.select(columns).toPandas()\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = df_pandas.corr()\n",
    "\n",
    "# 1️⃣ Heatmap for Correlation Matrix\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c61d4",
   "metadata": {},
   "source": [
    "**Why Remove dataset_index?\n",
    "    Irrelevance to Prediction**\n",
    "\n",
    "**The dataset index represents the original source of the data (e.g., Cleveland, Hungary, etc.). However, this categorical information may not contribute significantly to predicting heart disease.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27b9855f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- exang: integer (nullable = false)\n",
      " |-- fbs: integer (nullable = false)\n",
      " |-- id: long (nullable = true)\n",
      " |-- num: long (nullable = true)\n",
      " |-- sex: integer (nullable = false)\n",
      " |-- chol_imputed: double (nullable = true)\n",
      " |-- oldpeak_imputed: double (nullable = true)\n",
      " |-- thalch_imputed: double (nullable = true)\n",
      " |-- trestbps: double (nullable = true)\n",
      " |-- restecg_index: double (nullable = false)\n",
      " |-- cp_index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_indexed = df_indexed.drop(\"dataset_index\")\n",
    "df_indexed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501722bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# המרת DataFrame של PySpark ל-Pandas\n",
    "df_pandas = df_indexed.toPandas()\n",
    "\n",
    "# רשימת עמודות נומריות\n",
    "numeric_cols = ['oldpeak_imputed', 'thalch_imputed', 'chol_imputed', 'trestbps', 'age']\n",
    "\n",
    "# קביעת מספר שורות ועמודות עבור הסאבפלוטים\n",
    "num_cols = len(numeric_cols)\n",
    "num_rows = math.ceil(num_cols / 2)\n",
    "\n",
    "# הגדרת הצבעים\n",
    "colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'gray', 'brown']\n",
    "\n",
    "# יצירת הגרפים\n",
    "plt.figure(figsize=(20, num_rows * 5))\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    plt.subplot(num_rows, 2, i+1)  # חלוקה של הגרפים לשורות ועמודות\n",
    "    sns.boxplot(x=df_pandas[col], color=colors[i % len(colors)])\n",
    "    plt.title(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c74d71e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_indexed = df_indexed.filter(col(\"trestbps\") != 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f20c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Assemble features into a single vector column\n",
    "feature_columns = [col for col in df_indexed.columns if col != \"num\" and col != \"id\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df_vectorized = assembler.transform(df_indexed)\n",
    "\n",
    "# Step 2: Apply Min-Max Scaling\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"features_normalized\")\n",
    "df_normalized = scaler.fit(df_vectorized).transform(df_vectorized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8607ef8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "\n",
    "\n",
    "feature_columns = [col for col in df_indexed.columns if col != \"num\" and col != \"id\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"features_normalized\")\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "pipeline_model = pipeline.fit(df_indexed)\n",
    "df_normalized = pipeline_model.transform(df_indexed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e9ee6e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+---+---+---+------------+---------------+--------------+--------+-------------+--------+--------------------+--------------------+\n",
      "|age|exang|fbs| id|num|sex|chol_imputed|oldpeak_imputed|thalch_imputed|trestbps|restecg_index|cp_index|            features| features_normalized|\n",
      "+---+-----+---+---+---+---+------------+---------------+--------------+--------+-------------+--------+--------------------+--------------------+\n",
      "| 63|    0|  1|  1|  0|  1|       233.0|            2.3|         150.0|   145.0|          1.0|     3.0|[63.0,0.0,1.0,1.0...|[0.71428571428571...|\n",
      "| 67|    1|  0|  2|  2|  1|       286.0|            1.5|         108.0|   160.0|          1.0|     0.0|[67.0,1.0,0.0,1.0...|[0.79591836734693...|\n",
      "| 67|    1|  0|  3|  1|  1|       229.0|            2.6|         129.0|   120.0|          1.0|     0.0|[67.0,1.0,0.0,1.0...|[0.79591836734693...|\n",
      "| 37|    0|  0|  4|  0|  1|       250.0|            3.5|         187.0|   130.0|          0.0|     1.0|[37.0,0.0,0.0,1.0...|[0.18367346938775...|\n",
      "| 41|    0|  0|  5|  0|  0|       204.0|            1.4|         172.0|   130.0|          1.0|     2.0|[41.0,0.0,0.0,0.0...|[0.26530612244897...|\n",
      "| 56|    0|  0|  6|  0|  1|       236.0|            0.8|         178.0|   120.0|          0.0|     2.0|[56.0,0.0,0.0,1.0...|[0.57142857142857...|\n",
      "| 62|    0|  0|  7|  3|  0|       268.0|            3.6|         160.0|   140.0|          1.0|     0.0|[62.0,0.0,0.0,0.0...|[0.69387755102040...|\n",
      "| 57|    1|  0|  8|  0|  0|       354.0|            0.6|         163.0|   120.0|          0.0|     0.0|[57.0,1.0,0.0,0.0...|[0.59183673469387...|\n",
      "| 63|    0|  0|  9|  2|  1|       254.0|            1.4|         147.0|   130.0|          1.0|     0.0|[63.0,0.0,0.0,1.0...|[0.71428571428571...|\n",
      "| 53|    1|  1| 10|  1|  1|       203.0|            3.1|         155.0|   140.0|          1.0|     0.0|[53.0,1.0,1.0,1.0...|[0.51020408163265...|\n",
      "| 57|    0|  0| 11|  0|  1|       192.0|            0.4|         148.0|   140.0|          0.0|     0.0|[57.0,0.0,0.0,1.0...|[0.59183673469387...|\n",
      "| 56|    0|  0| 12|  0|  0|       294.0|            1.3|         153.0|   140.0|          1.0|     2.0|[56.0,0.0,0.0,0.0...|[0.57142857142857...|\n",
      "| 56|    1|  1| 13|  2|  1|       256.0|            0.6|         142.0|   130.0|          1.0|     1.0|[56.0,1.0,1.0,1.0...|[0.57142857142857...|\n",
      "| 44|    0|  0| 14|  0|  1|       263.0|            0.0|         173.0|   120.0|          0.0|     2.0|[44.0,0.0,0.0,1.0...|[0.32653061224489...|\n",
      "| 52|    0|  1| 15|  0|  1|       199.0|            0.5|         162.0|   172.0|          0.0|     1.0|[52.0,0.0,1.0,1.0...|[0.48979591836734...|\n",
      "| 57|    0|  0| 16|  0|  1|       168.0|            1.6|         174.0|   150.0|          0.0|     1.0|[57.0,0.0,0.0,1.0...|[0.59183673469387...|\n",
      "| 48|    0|  0| 17|  1|  1|       229.0|            1.0|         168.0|   110.0|          0.0|     2.0|[48.0,0.0,0.0,1.0...|[0.40816326530612...|\n",
      "| 54|    0|  0| 18|  0|  1|       239.0|            1.2|         160.0|   140.0|          0.0|     0.0|[54.0,0.0,0.0,1.0...|[0.53061224489795...|\n",
      "| 48|    0|  0| 19|  0|  0|       275.0|            0.2|         139.0|   130.0|          0.0|     1.0|[48.0,0.0,0.0,0.0...|[0.40816326530612...|\n",
      "| 49|    0|  0| 20|  0|  1|       266.0|            0.6|         171.0|   130.0|          0.0|     2.0|[49.0,0.0,0.0,1.0...|[0.42857142857142...|\n",
      "+---+-----+---+---+---+---+------------+---------------+--------------+--------+-------------+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_normalized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "572b4b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'exang',\n",
       " 'fbs',\n",
       " 'sex',\n",
       " 'chol_imputed',\n",
       " 'oldpeak_imputed',\n",
       " 'thalch_imputed',\n",
       " 'trestbps',\n",
       " 'restecg_index',\n",
       " 'cp_index']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "398445e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 582:===========================================>           (60 + 4) / 76]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 774 samples | Test Set: 145 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 582:====================================================>  (72 + 4) / 76]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.85\n",
    "test_ratio = 1 - train_ratio\n",
    "\n",
    "df_train, df_test = df_normalized.randomSplit([train_ratio, test_ratio], seed=42)\n",
    "\n",
    "#df_test, df_validation = df_test.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Step 2: Extract features (X) and labels (Y) for training and testing sets\n",
    "X_train = df_train.select(\"features_normalized\")\n",
    "y_train = df_train.select(col(\"num\").alias(\"label\"))\n",
    "\n",
    "#X_validation = df_validation.select(\"features_normalized\")\n",
    "#y_validation = df_validation.select(col(\"num\").alias(\"label\"))\n",
    "\n",
    "X_test = df_test.select(\"features_normalized\")\n",
    "y_test = df_test.select(col(\"num\").alias(\"label\"))\n",
    "\n",
    "# Step 3: Show the size of the training and testing sets\n",
    "train_count = df_train.count()\n",
    "#validation_count = df_validation.count()\n",
    "test_count = df_test.count()\n",
    "\n",
    "#print(f\"Training Set: {train_count} samples | Test Set: {test_count} samples | Validation Set: {validation_count} samples\")\n",
    "print(f\"Training Set: {train_count} samples | Test Set: {test_count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cac31b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxu/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 2.0.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/linuxu/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 212, in <module>\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 224, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 145, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 213, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 151, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1088, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1113, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 758, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 758, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 582, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1115, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 927, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import (\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 40, in <module>\n",
      "    from pyspark.ml.param import P\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 35, in <module>\n",
      "    from pyspark.ml.linalg import DenseVector, Vector, Matrix\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 81, in <module>\n",
      "    import scipy.sparse\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/scipy/sparse/__init__.py\", line 228, in <module>\n",
      "    from .csr import *\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/scipy/sparse/csr.py\", line 10, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n",
      "AttributeError: _ARRAY_API not found\n",
      "/home/linuxu/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 2.0.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/linuxu/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 212, in <module>\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 224, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 145, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 213, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 151, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1088, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1113, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 758, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 758, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 582, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1115, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 927, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import (\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 40, in <module>\n",
      "    from pyspark.ml.param import P\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 35, in <module>\n",
      "    from pyspark.ml.linalg import DenseVector, Vector, Matrix\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 81, in <module>\n",
      "    import scipy.sparse\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/scipy/sparse/__init__.py\", line 228, in <module>\n",
      "    from .csr import *\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/scipy/sparse/csr.py\", line 10, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n",
      "AttributeError: _ARRAY_API not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|vector_size|\n",
      "+-----------+\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "|         10|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxu/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 2.0.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/linuxu/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 212, in <module>\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 224, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 145, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 213, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 151, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1088, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1113, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 758, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 758, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 582, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1115, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 927, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import (\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 40, in <module>\n",
      "    from pyspark.ml.param import P\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 35, in <module>\n",
      "    from pyspark.ml.linalg import DenseVector, Vector, Matrix\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 81, in <module>\n",
      "    import scipy.sparse\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/scipy/sparse/__init__.py\", line 228, in <module>\n",
      "    from .csr import *\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/scipy/sparse/csr.py\", line 10, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n",
      "AttributeError: _ARRAY_API not found\n",
      "/home/linuxu/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 2.0.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/linuxu/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 212, in <module>\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 224, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 145, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 213, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 151, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1088, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1113, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 758, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 758, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 582, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1115, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 927, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import (\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 40, in <module>\n",
      "    from pyspark.ml.param import P\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 35, in <module>\n",
      "    from pyspark.ml.linalg import DenseVector, Vector, Matrix\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 81, in <module>\n",
      "    import scipy.sparse\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/scipy/sparse/__init__.py\", line 228, in <module>\n",
      "    from .csr import *\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/scipy/sparse/csr.py\", line 10, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n",
      "AttributeError: _ARRAY_API not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 587:================================================>      (67 + 4) / 76]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|vector_size(features_normalized)|\n",
      "+--------------------------------+\n",
      "|                              10|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "# פונקציה שמחזירה את מספר האלמנטים בווקטור\n",
    "def vector_size(v):\n",
    "    return len(v)\n",
    "\n",
    "# הגדרת UDF\n",
    "vector_size_udf = udf(vector_size, IntegerType())\n",
    "\n",
    "# הוספת עמודה חדשה עם אורך הווקטור\n",
    "df_train.select(vector_size_udf(\"features_normalized\").alias(\"vector_size\")).show()\n",
    "df_train.select(vector_size_udf(\"features_normalized\")).distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4207c41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features_normalized                                                                                                                                 |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[0.18367346938775508,0.0,0.0,1.0,0.41459369817578773,0.6931818181818181,0.8943661971830986,0.4166666666666667,0.0,0.3333333333333333]               |\n",
      "|[0.26530612244897955,0.0,0.0,0.0,0.3383084577114428,0.45454545454545453,0.7887323943661972,0.4166666666666667,0.3333333333333333,0.6666666666666666]|\n",
      "|[0.5102040816326531,1.0,1.0,1.0,0.33665008291873966,0.6477272727272727,0.6690140845070423,0.5,0.3333333333333333,0.0]                               |\n",
      "|[0.5714285714285714,0.0,0.0,1.0,0.3913764510779436,0.3863636363636364,0.8309859154929577,0.3333333333333333,0.0,0.6666666666666666]                 |\n",
      "|[0.5714285714285714,0.0,0.0,0.0,0.48756218905472637,0.44318181818181823,0.6549295774647887,0.5,0.3333333333333333,0.6666666666666666]               |\n",
      "|[0.5918367346938775,0.0,0.0,1.0,0.31840796019900497,0.3409090909090909,0.619718309859155,0.5,0.0,0.0]                                               |\n",
      "|[0.6938775510204082,0.0,0.0,0.0,0.4444444444444445,0.7045454545454546,0.7042253521126761,0.5,0.3333333333333333,0.0]                                |\n",
      "|[0.7142857142857142,0.0,1.0,1.0,0.3864013266998342,0.5568181818181819,0.6338028169014085,0.5416666666666666,0.3333333333333333,1.0]                 |\n",
      "|[0.7959183673469387,1.0,0.0,1.0,0.47429519071310117,0.46590909090909083,0.3380281690140845,0.6666666666666666,0.3333333333333333,0.0]               |\n",
      "|[0.7959183673469387,1.0,0.0,1.0,0.37976782752902155,0.5909090909090909,0.4859154929577465,0.3333333333333333,0.3333333333333333,0.0]                |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    0|\n",
      "|    0|\n",
      "|    1|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    3|\n",
      "|    0|\n",
      "|    2|\n",
      "|    1|\n",
      "+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "774"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.show(10, truncate=False)\n",
    "y_train.show(10)\n",
    "\n",
    "df_train.select(\"features_normalized\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b646c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.withColumnRenamed(\"num\", \"label\")\n",
    "df_test = df_test.withColumnRenamed(\"num\", \"label\")\n",
    "#df_validation = df_validation.withColumnRenamed(\"num\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9c3b50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.withColumn(\"label\", when(df_train[\"label\"] > 0, 1).otherwise(0))\n",
    "df_test = df_test.withColumn(\"label\", when(df_test[\"label\"] > 0, 1).otherwise(0))\n",
    "#df_validation = df_validation.withColumn(\"label\", when(df_validation[\"label\"] > 0, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "65b7302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 600:===========================================>           (60 + 4) / 76]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 774 samples | Test Set: 145 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 600:====================================================>  (72 + 4) / 76]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "train_count = df_train.count()\n",
    "#validation_count = df_validation.count()\n",
    "test_count = df_test.count()\n",
    "\n",
    "#print(f\"Training Set: {train_count} samples | Test Set: {test_count} samples | Validation Set: {validation_count} samples\")\n",
    "print(f\"Training Set: {train_count} samples | Test Set: {test_count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "399e07a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 10:04:47 WARN DAGScheduler: Broadcasting large task binary with size 1144.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 10:04:49 WARN DAGScheduler: Broadcasting large task binary with size 1493.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 10:04:51 WARN DAGScheduler: Broadcasting large task binary with size 1783.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 10:04:54 WARN DAGScheduler: Broadcasting large task binary with size 1994.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 10:04:57 WARN DAGScheduler: Broadcasting large task binary with size 1098.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 983:==============================================>        (64 + 4) / 76]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Selected: LogisticRegressionModel: uid=LogisticRegression_e03237a61534, numClasses=2, numFeatures=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 983:=====================================================> (74 + 2) / 76]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def compare_models(df_train, df_test, features_col=\"features_normalized\", label_col=\"label\"):\n",
    "    \"\"\"\n",
    "    Train and compare multiple classification models using train, test, and validation sets.\n",
    "\n",
    "    Args:\n",
    "        df_train (DataFrame): Training dataset.\n",
    "        df_test (DataFrame): Test dataset.\n",
    "        df_validation (DataFrame): Validation dataset.\n",
    "        features_col (str): Name of the column containing feature vectors.\n",
    "        label_col (str): Name of the target column.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A Spark DataFrame containing model performance results and the best trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "    # Define models for comparison\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(featuresCol=features_col, labelCol=label_col),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(featuresCol=features_col, labelCol=label_col),\n",
    "        \"Random Forest\": RandomForestClassifier(\n",
    "            featuresCol=features_col, labelCol=label_col, \n",
    "            numTrees=100, maxDepth=10, minInstancesPerNode=4, minInfoGain=0.0\n",
    "        ),\n",
    "        \"Gradient Boosted Trees\": GBTClassifier(\n",
    "    featuresCol=\"features_normalized\",\n",
    "    labelCol=\"label\",\n",
    "    maxDepth=3,           # עומק העצים\n",
    "    maxIter=50,           # מספר החזרות (מספר עצים)\n",
    "    stepSize=0.1,         # קצב הלמידה (learning_rate)\n",
    "    subsamplingRate=1.0,  # אחוז הדגימות לכל עץ\n",
    "    seed=42\n",
    ")\n",
    "    }\n",
    "\n",
    "    # Define the evaluator for accuracy\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=label_col, metricName=\"accuracy\")\n",
    "    results = []\n",
    "    best_model = None\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Train the model\n",
    "        trained_model = model.fit(df_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        predictions_test = trained_model.transform(df_test)\n",
    "        accuracy_test = evaluator.evaluate(predictions_test)\n",
    "\n",
    "        # Make predictions on the validation set\n",
    "       # predictions_validation = trained_model.transform(df_validation)\n",
    "        #accuracy_validation = evaluator.evaluate(predictions_validation)\n",
    "\n",
    "        # Store results\n",
    "        results.append((model_name, accuracy_test))\n",
    "\n",
    "        # Check if this is the best model so far\n",
    "        if accuracy_test > best_accuracy:\n",
    "            best_accuracy = accuracy_test\n",
    "            best_model = trained_model\n",
    "\n",
    "    # Convert results to a Spark DataFrame\n",
    "    results_df = spark.createDataFrame(results, [\"Model\", \"Test Accuracy\"])\n",
    "\n",
    "    return results_df, best_model\n",
    "\n",
    "# Run the function and get results + best model\n",
    "model_comparison_results, best_trained_model = compare_models(df_train, df_test)\n",
    "\n",
    "# Print the best model\n",
    "print(f\"Best Model Selected: {best_trained_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7035eb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/linuxu/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2863, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2909, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3106, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3309, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_192357/3097574556.py\", line 1, in <cell line: 1>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py\", line 109, in <module>\n",
      "    from . import _api, _version, cbook, docstring, rcsetup\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, cbook, scale\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/matplotlib/scale.py\", line 23, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/matplotlib/ticker.py\", line 136, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/matplotlib/transforms.py\", line 46, in <module>\n",
      "    from matplotlib._path import (\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# המרת הנתונים לפורמט Pandas\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py:109\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, docstring, rcsetup\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning, sanitize_sequence\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mplDeprecation  \u001b[38;5;66;03m# deprecated\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/rcsetup.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/colors.py:56\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook, scale\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_color_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ColorMapping\u001b[39;00m(\u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/scale.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, docstring\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     NullFormatter, ScalarFormatter, LogFormatterSciNotation, LogitFormatter,\n\u001b[1;32m     25\u001b[0m     NullLocator, LogLocator, AutoLocator, AutoMinorLocator,\n\u001b[1;32m     26\u001b[0m     SymmetricalLogLocator, LogitLocator)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transform, IdentityTransform\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mScaleBase\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/ticker.py:136\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m mtransforms\n\u001b[1;32m    138\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    140\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTickHelper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFixedFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    141\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNullFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuncFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatStrFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    142\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrMethodFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScalarFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultipleLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxNLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoMinorLocator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    149\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymmetricalLogLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogitLocator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/transforms.py:46\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inv\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_path\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     47\u001b[0m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     50\u001b[0m DEBUG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# המרת הנתונים לפורמט Pandas\n",
    "model_comparison_pandas = model_comparison_results.toPandas()\n",
    "\n",
    "# יצירת גרף השוואה\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# ציור דיוק המודלים על סט הבדיקה\n",
    "plt.barh(model_comparison_pandas[\"Model\"], model_comparison_pandas[\"Test Accuracy\"], alpha=0.6, label=\"Test Accuracy\")\n",
    "\n",
    "\n",
    "# הוספת כותרות וציר Y\n",
    "plt.xlabel(\"Accuracy Score\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.title(\"Model Comparison: Test vs Validation Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# הצגת הגרף\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "308e733a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|               Model|     Test Accuracy|\n",
      "+--------------------+------------------+\n",
      "| Logistic Regression|0.8344827586206897|\n",
      "|       Decision Tree|0.7724137931034483|\n",
      "|       Random Forest|0.8068965517241379|\n",
      "|Gradient Boosted ...|               0.8|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_comparison_results.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ff80f767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid=LogisticRegression_e03237a61534, numClasses=2, numFeatures=10"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b7bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0710a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from confluent_kafka import Consumer, Producer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, Normalizer\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c693c9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka Configuration\n",
    "consumer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'test-group',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "consumer = Consumer(consumer_config)\n",
    "consumer.subscribe(['test'])  # Subscribe to Kafka topic\n",
    "\n",
    "producer_config = {'bootstrap.servers': 'localhost:9092'}\n",
    "producer = Producer(producer_config)\n",
    "KAFKA_TOPIC_OUTPUT = \"prediction\"\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"KafkaConsumer\").getOrCreate()\n",
    "received_test_rdd = spark.sparkContext.emptyRDD()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7e3cc286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    \"\"\"\n",
    "    Convert RDD to DataFrame and rename columns.\n",
    "    \"\"\"\n",
    "    df = spark.createDataFrame(received_test_rdd)\n",
    "    rename_dict = {\n",
    "        'age': 'age',\n",
    "        'chol': 'chol_imputed',\n",
    "        'cp': 'cp_index',\n",
    "        'exang': 'exang',\n",
    "        'fbs': 'fbs',\n",
    "        'oldpeak': 'oldpeak_imputed',\n",
    "        'restecg': 'restecg_index',\n",
    "        'sex': 'sex',\n",
    "        'thalch': 'thalch_imputed',\n",
    "        'trestbps': 'trestbps'\n",
    "    }\n",
    "    for old_col, new_col in rename_dict.items():\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "    return df\n",
    "\n",
    "def normalize_features(df):\n",
    "    \"\"\"\n",
    "    Assemble and normalize feature vectors.\n",
    "    \"\"\"\n",
    "    feature_columns = [col for col in df.columns if col != \"num\" and col != \"id\"]\n",
    "    df_vectorized = pipeline_model.transform(df)\n",
    "    return df_vectorized\n",
    "\n",
    "def predict(df_normalized, model):\n",
    "    \"\"\"\n",
    "    Run the trained model on new data.\n",
    "    \"\"\"\n",
    "    return model.transform(df_normalized)\n",
    "\n",
    "def send_predictions_to_kafka(predictions):\n",
    "    \"\"\"\n",
    "    Sends only the first model prediction to Kafka topic.\n",
    "    \"\"\"\n",
    "    first_row = predictions.select(\"prediction\").first()\n",
    "    if first_row:\n",
    "        prediction_value = first_row[\"prediction\"]\n",
    "        prediction_data = {\"prediction\": float(prediction_value)}\n",
    "        producer.produce(KAFKA_TOPIC_OUTPUT, json.dumps(prediction_data).encode('utf-8'))\n",
    "        producer.flush()\n",
    "        print(f\"📤 Sent Single Prediction: {prediction_data}\")\n",
    "    else:\n",
    "        print(\"⚠ No prediction available to send!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "22bc4714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_messages2(best_trained_model):\n",
    "    \"\"\"\n",
    "    Kafka message consumer that processes incoming data, transforms it,\n",
    "    runs predictions, and sends results back to Kafka.\n",
    "    \"\"\"\n",
    "    global received_test_rdd\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            msg = consumer.poll(1.0)\n",
    "            if msg is None:\n",
    "                continue\n",
    "            if msg.error():\n",
    "                print(f\"Consumer error: {msg.error()}\")\n",
    "                continue\n",
    "            \n",
    "            data = json.loads(msg.value().decode('utf-8'))\n",
    "            received_test_rdd = spark.sparkContext.parallelize([data])  # Overwrite instead of append\n",
    "\n",
    "            \n",
    "            print(f\"✅ Received record: {data}, Total stored in RDD: {received_test_rdd.count()}\")\n",
    "            \n",
    "            # Process and predict\n",
    "            time.sleep(1)  # Suspension for 1 second\n",
    "            df = process_data()\n",
    "            df_normalized = normalize_features(df)\n",
    "            df_normalized.show()\n",
    "            predictions = predict(df_normalized, best_trained_model)\n",
    "            predictions.select(\"features_normalized\", \"prediction\").show(truncate=False)\n",
    "            \n",
    "            send_predictions_to_kafka(predictions)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Consumer interrupted. Closing connection...\")\n",
    "    finally:\n",
    "        consumer.close()\n",
    "        print(f\"📊 Total number of records received in RDD: {received_test_rdd.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc0460",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Received record: {'age': 15, 'sex': 1, 'trestbps': 180, 'chol': 100, 'fbs': 0, 'restecg': 2, 'thalch': 50, 'exang': 1, 'oldpeak': 0.0, 'cp': 0}, Total stored in RDD: 1\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 15|         100|       0|    1|  0|            0.0|            2|  1|            50|     180|[15.0,1.0,0.0,1.0...|[-0.2653061224489...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                                      |prediction|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[-0.26530612244897955,1.0,0.0,1.0,0.16583747927031509,0.29545454545454547,-0.07042253521126761,0.8333333333333334,0.6666666666666666,0.0]|1.0       |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n",
      "📤 Sent Single Prediction: {'prediction': 1.0}\n",
      "✅ Received record: {'age': 15, 'sex': 0, 'trestbps': 160, 'chol': 100, 'fbs': 0, 'restecg': 0, 'thalch': 50, 'exang': 1, 'oldpeak': 0.0, 'cp': 1}, Total stored in RDD: 1\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 15|         100|       1|    1|  0|            0.0|            0|  0|            50|     160|[15.0,1.0,0.0,0.0...|[-0.2653061224489...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                                      |prediction|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[-0.26530612244897955,1.0,0.0,0.0,0.16583747927031509,0.29545454545454547,-0.07042253521126761,0.6666666666666666,0.0,0.3333333333333333]|0.0       |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n",
      "📤 Sent Single Prediction: {'prediction': 0.0}\n",
      "✅ Received record: {'age': 78, 'sex': 0, 'trestbps': 170, 'chol': 100, 'fbs': 0, 'restecg': 0, 'thalch': 50, 'exang': 0, 'oldpeak': 0.0, 'cp': 1}, Total stored in RDD: 1\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 78|         100|       1|    0|  0|            0.0|            0|  0|            50|     170|(10,[0,4,6,7,9],[...|[1.02040816326530...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n",
      "+-------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                      |prediction|\n",
      "+-------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[1.0204081632653061,0.0,0.0,0.0,0.16583747927031509,0.29545454545454547,-0.07042253521126761,0.75,0.0,0.3333333333333333]|1.0       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n",
      "📤 Sent Single Prediction: {'prediction': 1.0}\n",
      "✅ Received record: {'age': 78, 'sex': 0, 'trestbps': 170, 'chol': 100, 'fbs': 0, 'restecg': 0, 'thalch': 50, 'exang': 0, 'oldpeak': 0.0, 'cp': 1}, Total stored in RDD: 1\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 78|         100|       1|    0|  0|            0.0|            0|  0|            50|     170|(10,[0,4,6,7,9],[...|[1.02040816326530...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n",
      "+-------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                      |prediction|\n",
      "+-------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[1.0204081632653061,0.0,0.0,0.0,0.16583747927031509,0.29545454545454547,-0.07042253521126761,0.75,0.0,0.3333333333333333]|1.0       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n",
      "📤 Sent Single Prediction: {'prediction': 1.0}\n",
      "✅ Received record: {'age': 78, 'sex': 0, 'trestbps': 170, 'chol': 100, 'fbs': 0, 'restecg': 0, 'thalch': 50, 'exang': 0, 'oldpeak': 0.0, 'cp': 1}, Total stored in RDD: 1\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 78|         100|       1|    0|  0|            0.0|            0|  0|            50|     170|(10,[0,4,6,7,9],[...|[1.02040816326530...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n",
      "+-------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                      |prediction|\n",
      "+-------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[1.0204081632653061,0.0,0.0,0.0,0.16583747927031509,0.29545454545454547,-0.07042253521126761,0.75,0.0,0.3333333333333333]|1.0       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n",
      "📤 Sent Single Prediction: {'prediction': 1.0}\n",
      "✅ Received record: {'age': 63, 'sex': 1, 'trestbps': 170, 'chol': 100, 'fbs': 0, 'restecg': 1, 'thalch': 50, 'exang': 0, 'oldpeak': 0.0, 'cp': 2}, Total stored in RDD: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 63|         100|       2|    0|  0|            0.0|            1|  1|            50|     170|[63.0,0.0,0.0,1.0...|[0.71428571428571...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                                     |prediction|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[0.7142857142857142,0.0,0.0,1.0,0.16583747927031509,0.29545454545454547,-0.07042253521126761,0.75,0.3333333333333333,0.6666666666666666]|1.0       |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n",
      "📤 Sent Single Prediction: {'prediction': 1.0}\n",
      "✅ Received record: {'age': 63, 'sex': 1, 'trestbps': 145, 'chol': 233, 'fbs': 1, 'restecg': 1, 'thalch': 50, 'exang': 0, 'oldpeak': 2.3, 'cp': 2}, Total stored in RDD: 1\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 63|         233|       2|    0|  1|            2.3|            1|  1|            50|     145|[63.0,0.0,1.0,1.0...|[0.71428571428571...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                                                 |prediction|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[0.7142857142857142,0.0,1.0,1.0,0.3864013266998342,0.5568181818181819,-0.07042253521126761,0.5416666666666666,0.3333333333333333,0.6666666666666666]|1.0       |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n",
      "📤 Sent Single Prediction: {'prediction': 1.0}\n",
      "✅ Received record: {'age': 63, 'sex': 1, 'trestbps': 145, 'chol': 233, 'fbs': 1, 'restecg': 1, 'thalch': 150, 'exang': 0, 'oldpeak': 2.3, 'cp': 2}, Total stored in RDD: 1\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 63|         233|       2|    0|  1|            2.3|            1|  1|           150|     145|[63.0,0.0,1.0,1.0...|[0.71428571428571...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                                               |prediction|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[0.7142857142857142,0.0,1.0,1.0,0.3864013266998342,0.5568181818181819,0.6338028169014085,0.5416666666666666,0.3333333333333333,0.6666666666666666]|1.0       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n",
      "📤 Sent Single Prediction: {'prediction': 1.0}\n",
      "✅ Received record: {'age': 63, 'sex': 1, 'trestbps': 145, 'chol': 233, 'fbs': 1, 'restecg': 1, 'thalch': 150, 'exang': 0, 'oldpeak': 2.3, 'cp': 2}, Total stored in RDD: 1\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 63|         233|       2|    0|  1|            2.3|            1|  1|           150|     145|[63.0,0.0,1.0,1.0...|[0.71428571428571...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                                               |prediction|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[0.7142857142857142,0.0,1.0,1.0,0.3864013266998342,0.5568181818181819,0.6338028169014085,0.5416666666666666,0.3333333333333333,0.6666666666666666]|1.0       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n",
      "📤 Sent Single Prediction: {'prediction': 1.0}\n",
      "✅ Received record: {'age': 37, 'sex': 1, 'trestbps': 130, 'chol': 250, 'fbs': 0, 'restecg': 0, 'thalch': 187, 'exang': 0, 'oldpeak': 3.5, 'cp': 1}, Total stored in RDD: 1\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 37|         250|       1|    0|  0|            3.5|            0|  1|           187|     130|[37.0,0.0,0.0,1.0...|[0.18367346938775...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                                  |prediction|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[0.18367346938775508,0.0,0.0,1.0,0.41459369817578773,0.6931818181818181,0.8943661971830986,0.4166666666666667,0.0,0.3333333333333333]|1.0       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤 Sent Single Prediction: {'prediction': 1.0}\n",
      "✅ Received record: {'age': 37, 'sex': 1, 'trestbps': 120, 'chol': 150, 'fbs': 0, 'restecg': 0, 'thalch': 187, 'exang': 0, 'oldpeak': 3.5, 'cp': 1}, Total stored in RDD: 1\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 37|         150|       1|    0|  0|            3.5|            0|  1|           187|     120|[37.0,0.0,0.0,1.0...|[0.18367346938775...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                                  |prediction|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[0.18367346938775508,0.0,0.0,1.0,0.24875621890547264,0.6931818181818181,0.8943661971830986,0.3333333333333333,0.0,0.3333333333333333]|1.0       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n",
      "📤 Sent Single Prediction: {'prediction': 1.0}\n",
      "✅ Received record: {'age': 37, 'sex': 1, 'trestbps': 120, 'chol': 150, 'fbs': 0, 'restecg': 0, 'thalch': 187, 'exang': 0, 'oldpeak': 3.5, 'cp': 0}, Total stored in RDD: 1\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 37|         150|       0|    0|  0|            3.5|            0|  1|           187|     120|[37.0,0.0,0.0,1.0...|[0.18367346938775...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n",
      "+----------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                   |prediction|\n",
      "+----------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[0.18367346938775508,0.0,0.0,1.0,0.24875621890547264,0.6931818181818181,0.8943661971830986,0.3333333333333333,0.0,0.0]|1.0       |\n",
      "+----------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n",
      "📤 Sent Single Prediction: {'prediction': 1.0}\n",
      "✅ Received record: {'age': 50, 'sex': 1, 'trestbps': 100, 'chol': 150, 'fbs': 0, 'restecg': 0, 'thalch': 187, 'exang': 0, 'oldpeak': 3.5, 'cp': 0}, Total stored in RDD: 1\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 50|         150|       0|    0|  0|            3.5|            0|  1|           187|     100|[50.0,0.0,0.0,1.0...|[0.44897959183673...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                    |prediction|\n",
      "+-----------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[0.44897959183673464,0.0,0.0,1.0,0.24875621890547264,0.6931818181818181,0.8943661971830986,0.16666666666666666,0.0,0.0]|1.0       |\n",
      "+-----------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n",
      "📤 Sent Single Prediction: {'prediction': 1.0}\n",
      "✅ Received record: {'age': 50, 'sex': 1, 'trestbps': 100, 'chol': 150, 'fbs': 0, 'restecg': 0, 'thalch': 187, 'exang': 0, 'oldpeak': 0.0, 'cp': 0}, Total stored in RDD: 1\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 50|         150|       0|    0|  0|            0.0|            0|  1|           187|     100|(10,[0,3,4,6,7],[...|[0.44897959183673...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n",
      "+------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                     |prediction|\n",
      "+------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[0.44897959183673464,0.0,0.0,1.0,0.24875621890547264,0.29545454545454547,0.8943661971830986,0.16666666666666666,0.0,0.0]|0.0       |\n",
      "+------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n",
      "📤 Sent Single Prediction: {'prediction': 0.0}\n",
      "✅ Received record: {'age': 50, 'sex': 1, 'trestbps': 100, 'chol': 150, 'fbs': 0, 'restecg': 0, 'thalch': 187, 'exang': 0, 'oldpeak': 0.0, 'cp': 0}, Total stored in RDD: 1\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 50|         150|       0|    0|  0|            0.0|            0|  1|           187|     100|(10,[0,3,4,6,7],[...|[0.44897959183673...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n",
      "+------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                     |prediction|\n",
      "+------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[0.44897959183673464,0.0,0.0,1.0,0.24875621890547264,0.29545454545454547,0.8943661971830986,0.16666666666666666,0.0,0.0]|0.0       |\n",
      "+------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n",
      "📤 Sent Single Prediction: {'prediction': 0.0}\n",
      "✅ Received record: {'age': 50, 'sex': 1, 'trestbps': 100, 'chol': 150, 'fbs': 0, 'restecg': 0, 'thalch': 187, 'exang': 0, 'oldpeak': 0.0, 'cp': 1}, Total stored in RDD: 1\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 50|         150|       1|    0|  0|            0.0|            0|  1|           187|     100|[50.0,0.0,0.0,1.0...|[0.44897959183673...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                                    |prediction|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[0.44897959183673464,0.0,0.0,1.0,0.24875621890547264,0.29545454545454547,0.8943661971830986,0.16666666666666666,0.0,0.3333333333333333]|0.0       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n",
      "📤 Sent Single Prediction: {'prediction': 0.0}\n",
      "✅ Received record: {'age': 50, 'sex': 1, 'trestbps': 100, 'chol': 150, 'fbs': 0, 'restecg': 0, 'thalch': 187, 'exang': 0, 'oldpeak': 0.1, 'cp': 1}, Total stored in RDD: 1\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "|age|chol_imputed|cp_index|exang|fbs|oldpeak_imputed|restecg_index|sex|thalch_imputed|trestbps|            features| features_normalized|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "| 50|         150|       1|    0|  0|            0.1|            0|  1|           187|     100|[50.0,0.0,0.0,1.0...|[0.44897959183673...|\n",
      "+---+------------+--------+-----+---+---------------+-------------+---+--------------+--------+--------------------+--------------------+\n",
      "\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|features_normalized                                                                                                                   |prediction|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[0.44897959183673464,0.0,0.0,1.0,0.24875621890547264,0.3068181818181818,0.8943661971830986,0.16666666666666666,0.0,0.3333333333333333]|0.0       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "\n",
      "📤 Sent Single Prediction: {'prediction': 0.0}\n"
     ]
    }
   ],
   "source": [
    "consume_messages2(best_trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d75f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
